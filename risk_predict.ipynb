{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''    Importing Libraries    ''' \n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Reading Data'''\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Drop Attributes\n",
    "\n",
    "Passing the data and corresponding attribute to remove\n",
    "\n",
    "'''\n",
    "def drop_attrib(X,attrib):\n",
    "    X.drop(attrib,inplace = True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Splitting the target column \"risk\". Removing Text Columns\n",
    "'''\n",
    "X = train.copy()\n",
    "\n",
    "\n",
    "def split_target(data):\n",
    "    z = data[\"risk\"]\n",
    "    drop_attrib(data,\"risk\")\n",
    "    return z\n",
    "\n",
    "y = split_target(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Making copy for Regressor and Classifier\n",
    "\n",
    "'''\n",
    "X_reg__ = X.copy()\n",
    "X_clf__ = X.copy()\n",
    "y_reg__ = y.copy()\n",
    "y_clf__ = y.copy()\n",
    "def classify_my_y(data):\n",
    "    data[data!=0] = 1   # For classifying Using One v/s Rest strategy\n",
    "classify_my_y(y_clf__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dropping data for Regressor Calculated using feature_importances_ of GradientBoostingRegressor \n",
    "'''\n",
    "\n",
    "def reg_remove_scale(dat):\n",
    "    data = dat.copy()\n",
    "    str_remove = [\"batt_instance\",\"batt_voltage\",\"installed_count\",\"event_country_code\",\"batt_manufacturer\"]\n",
    "    for i in range(len(str_remove)):\n",
    "        drop_attrib(data,str_remove[i])\n",
    "    return scale_data(data)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Dropping data for Classifier Calculated using feature_importances_ of GradientBoostingClassifier \n",
    "'''\n",
    "def clf_remove_scale(dat):\n",
    "    data = dat.copy()\n",
    "    str_remove = [\"batt_instance\",\"batt_voltage\",\"installed_count\",\"design_voltage\",\"event_country_code\",\"batt_manufacturer\"]\n",
    "    for i in range(len(str_remove)):\n",
    "        drop_attrib(data,str_remove[i])\n",
    "    return scale_data(data)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Scaling of Data\n",
    "\n",
    "'''\n",
    "def scale_data(data):\n",
    "    scaler = StandardScaler()\n",
    "    z = scaler.fit_transform(data)\n",
    "    return z\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Splitting Data for Training Cross Validation and Test\n",
    "'''\n",
    "\n",
    "X_reg = reg_remove_scale(X_reg__)\n",
    "X_clf = clf_remove_scale(X_clf__)\n",
    "\n",
    "X_real,X_test,y_real,y_test = train_test_split(X_reg,y_reg,test_size = 0.2,random_state = 20)\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_real,y_real,test_size = 0.2,random_state = 42)\n",
    "\n",
    "cX_real,cX_test,cy_real,cy_test = train_test_split(X_clf,y_clf,test_size = 0.2,random_state = 20)\n",
    "cX_train,cX_val,cy_train,cy_val = train_test_split(cX_real,cy_real,test_size = 0.2,random_state = 42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Pre-calculated Parameter Declaration For Regressor ran over multiple iteration of Cross Validations\n",
    "\n",
    "'''\n",
    "\n",
    "reg_impurity = 2\n",
    "reg_depth = 10\n",
    "reg_estimator = 377\n",
    "reg_split = 9\n",
    "reg_leaf = 8\n",
    "reg_learn = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Pre-calculated Parameter Declaration For Classifier ran over multiple iteration of Cross Validations\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "clf_depth = 8\n",
    "clf_estimator = 100\n",
    "clf_split = 9\n",
    "clf_leaf = 8\n",
    "clf_learn = 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nRegressor Parameter Calculation : \\n\\n0.Keep n_estimators as ~1000 for calculation of the below parameters\\n\\n1.max_depth: Fixing it as 10 because after this state it overfits \\n             using GridSearch always gives the maximum among all\\n             the depths passed so chose this after lot of iterations\\n             and cross validation.\\n\\n2.min_samples_split and min_samples_leaf : Do grid search for combination of [8,9] in both the parameters.\\n\\n3.min_impurity_decrease : GridSearch for [0,1,2,3,4] gives mostly  2 \\n\\n4.learning_rate: GridSearch for [0.]\\n\\n4.5 : Using the above parameters to calculate the optimal n_estimator\\n\\n5.n_estimator : Using early stopping strategy to calculate no. of trees estimators.  \\n\\n\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Regressor Parameter Calculation : \n",
    "\n",
    "0.Keep n_estimators as ~1000 for calculation of the below parameters\n",
    "\n",
    "1.max_depth: Fixing it as 10 because after this state it overfits \n",
    "             using GridSearch always gives the maximum among all\n",
    "             the depths passed so chose this after lot of iterations\n",
    "             and cross validation.\n",
    "\n",
    "2.min_samples_split and min_samples_leaf : Do grid search for combination of [8,9] in both the parameters.\n",
    "\n",
    "3.min_impurity_decrease : GridSearch for [0,1,2,3,4] gives mostly  2 \n",
    "\n",
    "4.learning_rate: GridSearch for [0.05,0.1,0.2,0.3] mostly gives 0.2 for this dataset No need to GridCheck !\n",
    "\n",
    "4.5 : Using the above parameters to calculate the optimal n_estimator\n",
    "\n",
    "5.n_estimator : Using early stopping strategy to calculate no. of trees estimators.  \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------Regressor Parameter Calculation Code------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "For  min_samples_split and min_samples_leaf\n",
    "'''\n",
    "\n",
    "\n",
    "gbrt_leaf_split = GradientBoostingRegressor(max_depth=10,n_estimators=1000,learning_rate=0.2)\n",
    "param_grid = { \"min_samples_split\" : [8,9],\n",
    "              \"min_samples_leaf\" : [8,9]\n",
    "              \n",
    "        \n",
    "        }\n",
    "CV_split_leaf = GridSearchCV(gbrt_leaf_split, param_grid=param_grid, cv= 3,scoring = 'neg_mean_squared_error')\n",
    "CV_split_leaf.fit(X_real,y_real )\n",
    "leaf_split = CV_split_leaf.best_params_\n",
    "reg_split = leaf_split['min_samples_splits']\n",
    "reg_leaf = leaf_split['min_samples_leaf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "For  min_impurity_decrease\n",
    "'''\n",
    "\n",
    "\n",
    "gbrt_impure = GradientBoostingRegressor(max_depth=10,n_estimators=1000,learning_rate=0.2,min_samples_leaf=reg_leaf,min_samples_split=reg_split)\n",
    "param_grid = { \n",
    "              \"min_impurity_decrease\" : [0,1,2,3,4]\n",
    "              \n",
    "        \n",
    "        }\n",
    "CV_impure = GridSearchCV(gbrt_impure, param_grid=param_grid, cv= 3,scoring = 'neg_mean_squared_error')\n",
    "CV_impure.fit(X_real,y_real )\n",
    "impure = CV_impure.best_params_\n",
    "reg_impurity = impure['min_impurity_decrease']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using Early stopping strategy to calculate no. of estimators \n",
    "required in order to decrease computations and overfitting\n",
    "\n",
    "'''\n",
    "\n",
    "gbrt_est = GradientBoostingRegressor(min_impurity_decrease=reg_impurity,max_depth=reg_depth,min_samples_leaf = reg_leaf,min_samples_split=reg_split,warm_start = True,learning_rate = reg_learn)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "n_estimators = 0\n",
    "for n_estimators in range(1,1000):\n",
    "    gbrt_est.n_estimators = n_estimators\n",
    "    gbrt_est.fit(X_real,y_real)\n",
    "    y_pred = gbrt_est.predict(X_test)\n",
    "    val_error = mean_squared_error(y_test,y_pred)\n",
    "    if val_error<min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up +=1\n",
    "        if error_going_up==10:\n",
    "            break ;\n",
    "\n",
    "# increasing estimator\n",
    "n_estimators = (math.ceil(n_estimators/100))*100\n",
    "\n",
    "reg_estimator = n_estimators            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Classifier Parameter Calculation : \n",
    "\n",
    "0.Keep n_estimators as ~1000 for calculation of the below parameters\n",
    "\n",
    "1.max_depth: Fixing it as 8 because after this state it overfits \n",
    "             using GridSearch always gives the maximum among all\n",
    "             the depths passed so chose this after lot of iterations\n",
    "             and cross validation.\n",
    "\n",
    "2.min_samples_split and min_samples_leaf : Do grid search for combination of [8,9] in both the parameters.\n",
    "\n",
    "3.learning_rate: GridSearch for [0.09,,0.1,0.2] mostly gives 0.09 for this dataset No need to GridCheck !\n",
    "\n",
    "3.5 : Using the above parameters to calculate the optimal n_estimator\n",
    "\n",
    "4.n_estimator : Using early stopping strategy to calculate no. of trees estimators.  \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#-------Classifier Parameter Calculation Code------#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "For  min_samples_split and min_samples_leaf\n",
    "'''\n",
    "\n",
    "\n",
    "c_gbrt_leaf_split = GradientBoostingClassifier(max_depth=8,n_estimators=1000,learning_rate=0.09)\n",
    "param_grid = { \"min_samples_split\" : [8,9],\n",
    "              \"min_samples_leaf\" : [8,9]\n",
    "              \n",
    "        \n",
    "        }\n",
    "c_CV_split_leaf = GridSearchCV(c_gbrt_leaf_split, param_grid=param_grid, cv= 3,scoring = 'neg_mean_squared_error')\n",
    "c_CV_split_leaf.fit(cX_real,cy_real )\n",
    "c_leaf_split = c_CV_split_leaf.best_params_\n",
    "clf_split = c_leaf_split['min_samples_splits']\n",
    "clf_leaf = c_leaf_split['min_samples_leaf']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Using Early stopping strategy to calculate no. of estimators \n",
    "required in order to decrease computations and overfitting\n",
    "\n",
    "'''\n",
    "\n",
    "c_gbrt_est = GradientBoostingClassifier(max_depth=8,min_samples_split=9,min_samples_leaf=8,learning_rate=0.09,warm_start=True)\n",
    "min_val_error = float(\"inf\")\n",
    "error_going_up = 0\n",
    "n_estimators = 0\n",
    "for n_estimators in range(1,1200):\n",
    "    c_gbrt_est.n_estimators = n_estimators\n",
    "    c_gbrt_est.fit(cX_real_drop,cy_real_drop)\n",
    "    y_pred = c_gbrt_est.predict(cX_test_drop)\n",
    "    val_error = mean_squared_error(cy_test_drop,y_pred)\n",
    "    if val_error<min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up +=1\n",
    "        if error_going_up==15: \n",
    "            break \n",
    "# A minimum of 100 estimators for the classifier            \n",
    "if n_estimators<100:\n",
    "    n_estimators = 100\n",
    "clf_estimator = n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Function : \n",
    "            Sets the risk = 0 values calculated by classifier in the respective predictions made by the \n",
    "            regressor\n",
    "            \n",
    "            Also rounds off predictions\n",
    "            \n",
    "            Fills up predictions which are negative and classifier says risk!=0 with the average\n",
    "            calculated in this case with diff combination and diff random seeds in train,val and \n",
    "            test set.\n",
    "\n",
    "'''\n",
    "\n",
    "def precise_pred(reg_pred,clf_pred,avg):\n",
    "    reg =reg_pred.copy() \n",
    "    clf = clf_pred.copy()\n",
    "    reg[clf==0] = 0\n",
    "    reg[(reg>0) & (reg<=0.5)] = 1\n",
    "    reg[(reg<0) & (clf!=0)] = avg # Filling the negative predictions made by regressor with average values in the case where my cross validation test used to give negative predictions instead of positive value\n",
    "    for i in range(len(reg)):\n",
    "        \n",
    "        reg[i] = round(reg[i])\n",
    "    \n",
    "    return reg\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of the average\n",
    "\n",
    "def cal_avg(y_re,reg,clf):\n",
    "    y_real = np.array(y_re)\n",
    "    count = 0\n",
    "    av = 0\n",
    "    for i in range(len(y_real)):\n",
    "        if (-y_real[i]<0) and (y_real[i]!=0) and (clf[i]!=0) and (round(reg[i])<=0):\n",
    "            av = av + y_real[i]\n",
    "            count+=1\n",
    "    return round((av/count))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "Returns the Model of Regressor and Classifier after fitting data\n",
    "\n",
    "NOTE: If new training data is feeded grid search for the best parameter first as \n",
    "      variables declared above will be used else default calculated paramter would\n",
    "      be used\n",
    "'''\n",
    "\n",
    "def reg_clf_model_fit(reg_train,reg_y,clf_train,clf_y):\n",
    "    gbrt_reg = GradientBoostingRegressor(n_estimators=reg_estimator,max_depth=reg_depth,min_impurity_decrease=reg_impurity,min_samples_leaf=reg_leaf,min_samples_split=reg_split,learning_rate=reg_learn)\n",
    "    gbrt_clf = GradientBoostingClassifier(n_estimators=clf_estimator,max_depth=clf_depth,min_samples_leaf=clf_leaf,min_samples_split=clf_split,learning_rate=clf_learn)\n",
    "    \n",
    "    gbrt_reg.fit(reg_train,reg_y)\n",
    "    gbrt_clf.fit(clf_train,clf_y)\n",
    "    return gbrt_reg,gbrt_clf\n",
    "    \n",
    "def svc_model_fit(clf_train,clf_y):\n",
    "    rbf = SVC(kernel = \"rbf\",gamma=0.096,C=100)\n",
    "       \n",
    "    rbf.fit(clf_train,clf_y)\n",
    "    return rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Returns the Prediction of Regressor and Classifier\n",
    "\n",
    "'''\n",
    "\n",
    "def model_prediction(reg_model,clf_model,reg_data,clf_data):\n",
    "    return reg_model.predict(reg_data),clf_model.predict(clf_data)\n",
    "\n",
    "def svc_pred(rbf_model,rbf_data):\n",
    "    return rbf_model.predict(rbf_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Average Calculation Process\n",
    "'''\n",
    "REG,CLF = reg_clf_model_fit(X_real,y_real,cX_real,cy_real)\n",
    "reg_pred_1,clf_pred_1 = model_prediction(REG,CLF,X_test,cX_test)\n",
    "av1 = cal_avg(y_test,reg_pred_1,clf_pred_1)\n",
    "REG,CLF = reg_clf_model_fit(X_train,y_train,cX_train,cy_train)\n",
    "reg_pred_1,clf_pred_1 = model_prediction(REG,CLF,X_val,cX_val)\n",
    "av2 = cal_avg(y_val,reg_pred_1,clf_pred_1)\n",
    "REG,CLF = reg_clf_model_fit(X_train,y_train,cX_train,cy_train)\n",
    "reg_pred_1,clf_pred_1 = model_prediction(REG,CLF,X_test,cX_test)\n",
    "av3 = cal_avg(y_test,reg_pred_1,clf_pred_1)\n",
    "av = (av1+av2+av3)/3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FINAL  PREDICTIONS ###\n",
    "\n",
    "test_reg = reg_remove_scale(test)\n",
    "test_clf = clf_remove_scale(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "REG_test,CLF_test = reg_clf_model_fit(X_reg,y,X_clf,y_clf)\n",
    "reg_test_pred,clf_test_pred = model_prediction(REG_test,CLF_test,test_reg,test_clf)\n",
    "SVC_test = svc_model_fit(X_clf,y_clf)\n",
    "svc_test_pred = svc_pred(SVC_test,test_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = precise_pred(reg_test_pred,clf_test_pred,av)\n",
    "svc_output = precise_pred(reg_test_pred,svc_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
